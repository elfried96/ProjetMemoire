"""
Wrapper pour KIM - Mod√®le VLM avanc√© pour la surveillance (pr√™t mais d√©sactiv√©).
√Ä activer quand les ressources GPU seront disponibles.
"""

import torch
import gc
from typing import List, Optional, Dict, Any
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image

from .base import BaseVLMModel, AnalysisResult
from ..config import settings
from ..utils.logging import get_surveillance_logger

logger = get_surveillance_logger()


class KIMWrapper(BaseVLMModel):
    """
    Wrapper pour le mod√®le KIM (d√©sactiv√© par d√©faut).
    
    Note: Ce mod√®le n√©cessite plus de ressources GPU que SmolVLM.
    Activez-le uniquement si vous disposez d'une GPU avec au moins 8GB VRAM.
    """
    
    def __init__(self, model_name: Optional[str] = None, device: Optional[str] = None):
        if model_name is None:
            # Utilise un mod√®le Kosmos-2 comme placeholder pour KIM
            model_name = "microsoft/kosmos-2-patch14-224"
            
        super().__init__(model_name, device)
        self.torch_dtype = torch.float16 if self.device == "cuda" else torch.float32
        
        # V√©rification des ressources avant chargement
        self._check_resources()
        
        if not settings.get_model_config(settings.ModelType.KIM).enabled:
            logger.info("KIM est d√©sactiv√© dans la configuration")
            return
            
        # Chargement diff√©r√© par d√©faut pour √©conomiser les ressources
        if settings.config.cleanup_after_analysis:
            logger.info(f"KIM initialis√© (chargement diff√©r√©): {model_name}")
        else:
            logger.warning("KIM sera charg√© imm√©diatement - v√©rifiez vos ressources GPU")
            self.load_model()
    
    def _check_resources(self) -> None:
        """V√©rifie la disponibilit√© des ressources GPU."""
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_memory_gb = gpu_memory / (1024**3)
            
            if gpu_memory_gb < 6:
                logger.warning(
                    f"GPU avec {gpu_memory_gb:.1f}GB d√©tect√©e. "
                    f"KIM recommande au moins 8GB VRAM."
                )
        else:
            logger.warning("Aucun GPU d√©tect√©. KIM sera tr√®s lent sur CPU.")
    
    def load_model(self) -> None:
        """Charge le mod√®le KIM en m√©moire."""
        if self._is_loaded:
            return
            
        if not settings.get_model_config(settings.ModelType.KIM).enabled:
            raise RuntimeError(
                "KIM est d√©sactiv√©. Activez-le avec settings.enable_model(ModelType.KIM)"
            )
            
        try:
            logger.info(f"Chargement de KIM: {self.model_name}")
            logger.warning("KIM n√©cessite beaucoup de VRAM - surveillez l'utilisation m√©moire")
            
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=self.torch_dtype,
                device_map="auto" if self.device == "cuda" else None,
                # Options d'optimisation m√©moire
                load_in_8bit=True if self.device == "cuda" else False,
                attn_implementation="flash_attention_2" if torch.cuda.is_available() else None
            )
            
            if self.device != "auto" and not self.device == "cuda":
                self.model = self.model.to(self.device)
            
            self._is_loaded = True
            logger.info(f"‚úÖ KIM charg√© avec succ√®s sur {self.device}")
            
            # Affiche l'utilisation m√©moire
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / (1024**3)
                logger.info(f"M√©moire GPU utilis√©e: {memory_used:.2f}GB")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de KIM: {e}")
            logger.info("üí° Conseil: Essayez SmolVLM si KIM √©choue")
            raise
    
    def unload_model(self) -> None:
        """D√©charge le mod√®le de la m√©moire."""
        if not self._is_loaded:
            return
            
        try:
            logger.info("D√©chargement de KIM...")
            del self.model
            del self.processor
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            
            self.model = None
            self.processor = None
            self._is_loaded = False
            logger.info("‚úÖ KIM d√©charg√©, m√©moire lib√©r√©e")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du d√©chargement: {e}")
    
    def analyze_images(
        self, 
        images: List[Image.Image], 
        prompt: str,
        max_new_tokens: int = 1024,
        temperature: float = 0.7,
        **kwargs
    ) -> AnalysisResult:
        """
        Analyse une liste d'images avec KIM.
        
        Args:
            images: Liste d'images PIL √† analyser
            prompt: Prompt textuel pour l'analyse
            max_new_tokens: Nombre maximum de tokens √† g√©n√©rer
            temperature: Temp√©rature pour la g√©n√©ration
            **kwargs: Arguments suppl√©mentaires
            
        Returns:
            R√©sultat d'analyse structur√©
        """
        self.ensure_loaded()
        
        if not images:
            logger.warning("Aucune image fournie pour l'analyse")
            return AnalysisResult()
        
        try:
            logger.info(f"Analyse de {len(images)} images avec KIM")
            
            inputs = self.processor(
                images=images,
                text=prompt,
                return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    # Optimisations pour r√©duire l'usage m√©moire
                    use_cache=False
                )

            # Extraire seulement les nouveaux tokens
            new_tokens = generated_ids[:, inputs.input_ids.shape[1]:]
            decoded = self.processor.batch_decode(
                new_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]

            thinking, summary = self._extract_thinking_and_summary(decoded)
            
            result = AnalysisResult(
                thinking=thinking,
                summary=summary,
                raw_output=decoded,
                confidence=self._estimate_confidence(decoded),
                metadata={
                    "model": "KIM",
                    "num_images": len(images),
                    "temperature": temperature,
                    "max_tokens": max_new_tokens
                }
            )
            
            logger.info("‚úÖ Analyse KIM termin√©e")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse KIM: {e}")
            return AnalysisResult(
                summary=f"Erreur d'analyse KIM: {str(e)}",
                metadata={"error": True, "error_message": str(e)}
            )
    
    def _extract_thinking_and_summary(self, text: str, bot: str = "‚óÅthink‚ñ∑", eot: str = "‚óÅ/think‚ñ∑") -> tuple:
        """
        Extrait les sections de r√©flexion du texte g√©n√©r√© par KIM.
        
        Args:
            text: Texte g√©n√©r√© par le mod√®le
            bot: Balise de d√©but de r√©flexion
            eot: Balise de fin de r√©flexion
            
        Returns:
            Tuple (thinking, summary)
        """
        if bot in text and eot in text:
            try:
                thinking_start = text.find(bot) + len(bot)
                thinking_end = text.find(eot)
                thinking = text[thinking_start:thinking_end].strip()
                summary = text[thinking_end + len(eot):].strip()
                return thinking, summary
            except Exception as e:
                logger.warning(f"Erreur extraction thinking/summary: {e}")
        
        # Fallback: tout le texte comme r√©sum√©
        return "", text.strip()
    
    def _estimate_confidence(self, text: str) -> float:
        """
        Estime un score de confiance bas√© sur le texte g√©n√©r√©.
        KIM √©tant plus avanc√©, il peut fournir des indices de confiance explicites.
        
        Args:
            text: Texte g√©n√©r√©
            
        Returns:
            Score de confiance entre 0 et 1
        """
        # Indicateurs de confiance sp√©cifiques √† KIM
        confidence_indicators = [
            ("tr√®s certain", 0.95), ("absolument", 0.95), ("d√©finitivement", 0.9),
            ("clairement", 0.85), ("certainement", 0.8), ("probablement", 0.7),
            ("il semble", 0.6), ("possiblement", 0.5), ("peut-√™tre", 0.4),
            ("incertain", 0.3), ("difficile √† dire", 0.2)
        ]
        
        text_lower = text.lower()
        max_confidence = 0.6  # Confiance par d√©faut plus √©lev√©e pour KIM
        
        for indicator, conf in confidence_indicators:
            if indicator in text_lower:
                max_confidence = max(max_confidence, conf)
        
        return max_confidence
    
    def _build_surveillance_prompt(self, section: str, time_of_day: str, crowd_density: str) -> str:
        """Prompt sp√©cialis√© pour KIM avec plus de d√©tails."""
        return f"""
Vous √™tes un syst√®me de surveillance avanc√© utilisant l'intelligence artificielle KIM. Analysez ces images de magasin avec pr√©cision.

CONTEXTE D√âTAILL√â:
- Section: {section}
- Heure: {time_of_day}
- Affluence: {crowd_density}

ANALYSE REQUISE:
1. Identification pr√©cise des personnes et objets
2. Analyse comportementale d√©taill√©e
3. D√©tection de patterns suspects
4. √âvaluation des risques contextuels
5. Recommandations d'action

CRIT√àRES DE SUSPICION AVANC√âS:
- Micro-expressions de stress ou nervosit√©
- Patterns de mouvement inhabituels
- Interactions sociales suspectes
- Manipulation d'objets non conforme
- Signaux visuels de dissimulation
- Comportements pr√©-criminels

INSTRUCTIONS SP√âCIALIS√âES:
- Utilisez votre capacit√© d'analyse avanc√©e
- Fournissez des d√©tails pr√©cis et contextuels
- Indiquez votre niveau de certitude
- Sugg√©rez des outils compl√©mentaires si n√©cessaire

Format de r√©ponse avec r√©flexion:
‚óÅthink‚ñ∑
[Analyse d√©taill√©e √©tape par √©tape]
‚óÅ/think‚ñ∑

[R√©sum√© ex√©cutif pour l'agent de s√©curit√©]
        """.strip()
    
    def cleanup(self) -> None:
        """Alias pour unload_model pour compatibilit√©."""
        self.unload_model()
    
    @classmethod
    def is_available(cls) -> bool:
        """V√©rifie si KIM peut √™tre utilis√© dans l'environnement actuel."""
        return (
            settings.get_model_config(settings.ModelType.KIM).enabled and
            torch.cuda.is_available() and
            torch.cuda.get_device_properties(0).total_memory > 6 * (1024**3)
        )