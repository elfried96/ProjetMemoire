# src/models/kim_wrapper.py

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM
from utils.preprocessing import preprocess_frame
import cv2
import gc

class KIMWrapper:
    def __init__(self, model_name="moonshotai/Kimi-VL-A3B-Thinking-2506", device=None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            load_in_8bit=True
        ).to(self.device)
        self.prompt_template = """
Vous √™tes un syst√®me de surveillance intelligente. Analysez cette image de magasin.
CONTEXTE: {store_section}, {time_of_day}, {crowd_density}
T√ÇCHES:
1. Identifiez toutes les personnes et leurs actions
2. D√©tectez tout comportement inhabituel ou suspect
3. D√©terminez quels outils utiliser pour validation:
- object_detection: pour identifier les objets manipul√©s
- movement_analysis: pour analyser les d√©placements
- context_validation: pour valider dans le contexte du magasin
CRIT√àRES SUSPECTS:
- Dissimulation d'objets
- Regards furtifs r√©p√©t√©s
- Mouvements vers les sorties sans passer en caisse
- Manipulation d'√©tiquettes/emballages
R√âPONSE ATTENDUE:
{{
"analysis": "description d√©taill√©e",
"suspicion_level": "low/medium/high",
"recommended_tools": ["tool1", "tool2"],
"reasoning": "justification"
}}
        """.strip()

    def extract_frames(self, video_path, seconds_per_frame=2):
        vidcap = cv2.VideoCapture(video_path)
        fps = vidcap.get(cv2.CAP_PROP_FPS)
        frame_interval = int(fps * seconds_per_frame)

        frames = []
        success, image = vidcap.read()
        count = 0
        while success:
            if count % frame_interval == 0:
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(rgb_image)
                preprocessed_image = preprocess_frame(pil_image)
                frames.append(preprocessed_image)
            success, image = vidcap.read()
            count += 1

        vidcap.release()
        print(f"‚úÖ {len(frames)} images extraites de la vid√©o.")
        return frames

    def generate_prompt(self, section, time_of_day, crowd_density):
        return self.prompt_template.format(
            store_section=section,
            time_of_day=time_of_day,
            crowd_density=crowd_density
        )

    def extract_thinking_and_summary(self, text, bot="‚óÅthink‚ñ∑", eot="‚óÅ/think‚ñ∑"):
        if eot in text:
            thinking = text[text.find(bot) + len(bot):text.find(eot)].strip()
            summary = text[text.find(eot) + len(eot):].strip()
            return thinking, summary
        return "", text

    def analyze_video(self, video_path, section, time_of_day, crowd_density):
        try:
            frames = self.extract_frames(video_path)
            if not frames:
                print("‚ùå Aucune frame extraite.")
                return None

            prompt = self.generate_prompt(section, time_of_day, crowd_density)

            messages = [{
                "role": "user",
                "content": [{"type": "image"} for _ in frames] + [
                    {"type": "text", "text": prompt}
                ],
            }]

            # G√©n√©ration des inputs
            text_input = self.processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
            inputs = self.processor(images=frames, text=text_input, return_tensors="pt").to(self.device)

            # Inf√©rence du mod√®le
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=1024, temperature=0.7)

            trimmed_ids = generated_ids[:, inputs.input_ids.shape[1]:]
            decoded = self.processor.batch_decode(
                trimmed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]

            thinking, summary = self.extract_thinking_and_summary(decoded)
            return {
                "thinking": thinking,
                "summary": summary,
                "raw": decoded
            }

        finally:
            # Lib√©rer proprement la m√©moire
            self.cleanup()

    def cleanup(self):
        print("üßπ Lib√©ration des ressources...")
        del self.model
        torch.cuda.empty_cache()
        gc.collect()
        print("‚úÖ M√©moire GPU/CPU nettoy√©e.")
