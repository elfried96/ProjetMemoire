# üì• Information sur le T√©l√©chargement des Mod√®les

## Comment √ßa fonctionne

### üîÑ T√©l√©chargement Automatique
Lors du **premier lancement**, les mod√®les sont t√©l√©charg√©s automatiquement via Hugging Face :

1. **SmolVLM** (`HuggingFaceTB/SmolVLM-Instruct`) - ~1.5 GB
2. **Phi-3** (`microsoft/phi-2`) - ~2.7 GB
3. **KIM** (`microsoft/kosmos-2-patch14-224`) - ~1.6 GB (optionnel)

### üìÇ Stockage Local
Les mod√®les sont mis en cache dans :
```
~/.cache/huggingface/hub/
‚îú‚îÄ‚îÄ models--HuggingFaceTB--SmolVLM-Instruct/
‚îú‚îÄ‚îÄ models--microsoft--phi-2/
‚îî‚îÄ‚îÄ models--microsoft--kosmos-2-patch14-224/
```

### ‚è±Ô∏è Temps de T√©l√©chargement (estim√©)
- **Connexion rapide (100 Mbps)** : 5-10 minutes
- **Connexion normale (20 Mbps)** : 15-30 minutes  
- **Connexion lente (5 Mbps)** : 45-90 minutes

### üöÄ Optimisations dans le Code

#### Chargement Diff√©r√© (par d√©faut)
```python
# Les mod√®les ne se chargent QUE quand n√©cessaires
if settings.config.cleanup_after_analysis:
    logger.info("Mod√®le initialis√© (chargement diff√©r√©)")
else:
    self.load_model()  # Chargement imm√©diat
```

#### Nettoyage Automatique
```python
# Lib√®re la m√©moire apr√®s chaque analyse
if settings.config.cleanup_after_analysis:
    self._cleanup_models()
```

## üõ†Ô∏è Contr√¥le Manuel du T√©l√©chargement

### Pr√©-t√©l√©charger les mod√®les
```python
from transformers import AutoModelForCausalLM, AutoProcessor

# Force le t√©l√©chargement de SmolVLM
processor = AutoProcessor.from_pretrained("HuggingFaceTB/SmolVLM-Instruct")
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolVLM-Instruct")

# Force le t√©l√©chargement de Phi-3
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")
model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2")
```

### Variables d'Environnement
```bash
# Cache personnalis√©
export TRANSFORMERS_CACHE="/mon/cache/custom"

# Offline mode (utilise seulement les mod√®les d√©j√† t√©l√©charg√©s)
export TRANSFORMERS_OFFLINE=1
```

## üéØ Premier Lancement Recommand√©

### √âtape 1 : Installation
```bash
pip install torch torchvision transformers accelerate
```

### √âtape 2 : Test de t√©l√©chargement
```bash
# Lance le t√©l√©chargement + test
python main.py --list-models
```

### √âtape 3 : Premi√®re analyse
```bash
# Utilise SmolVLM (plus l√©ger)
python main.py videos/surveillance_test.mp4 --verbose
```

## üîß Gestion des Erreurs de T√©l√©chargement

### Erreur r√©seau
```python
# Le code g√®re automatiquement les reprises
try:
    model = AutoModelForCausalLM.from_pretrained(model_name)
except Exception as e:
    logger.error(f"Erreur t√©l√©chargement: {e}")
    # Fallback ou nouvelle tentative
```

### Espace disque insuffisant
- **SmolVLM + Phi-3** : ~4.5 GB requis minimum
- **Avec KIM** : ~6 GB total

### Mode Offline
Si d√©j√† t√©l√©charg√©, fonctionne sans connexion :
```bash
export TRANSFORMERS_OFFLINE=1
python main.py videos/test.mp4
```

## üìä Monitoring du T√©l√©chargement

Le projet affiche la progression :
```
üîÑ T√©l√©chargement de SmolVLM: HuggingFaceTB/SmolVLM-Instruct
Downloading... ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% 1.2GB/1.2GB
‚úÖ SmolVLM charg√© avec succ√®s sur cuda
```

## üéõÔ∏è Configuration Avanc√©e

### Chargement en 8-bit (√©conomise la m√©moire)
```python
# Automatique pour KIM
load_in_8bit=True if self.device == "cuda" else False
```

### Device mapping automatique
```python
# R√©partit automatiquement sur GPU disponible
device_map="auto" if self.device == "cuda" else None
```